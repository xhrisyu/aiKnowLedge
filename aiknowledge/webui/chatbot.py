"""
Chatbot Page

$ Retrieving documents use qdrant client directly, not through FastAPI backend [http request is slow]
$ QA request use llm client directly, not through FastAPI backend [streaming output is hard to implement]
"""
import time
import streamlit as st

from aiknowledge.config import app_config
from aiknowledge.config.log_config import setup_logging
from aiknowledge.llm import OpenAILLM
from aiknowledge.db import KBQdrantClient, KBMongoClient
from aiknowledge.rag.retriever.bm25 import BM25
from aiknowledge.rag.retriever.rerank import Rerank
from aiknowledge.rag.query_analysis.query_analysis import query_analysis_pipeline, QueryType
from aiknowledge.webui.constants import LUCENE_INDEX_DIR_INTFLEX_AUDIT_CHUNK_DATA
from aiknowledge.rag.retrieval_augmentation import retrieval_augmentation_pipeline
from aiknowledge.rag.context_prompt import format_context_prompt, format_qa_history_prompt


logger = setup_logging()


vector_search_params = {
    "chunk_data": {
        "qdrant_collection_name": "intflex_audit",
        "mongo_database_name": "intflex_audit",
        "mongo_collection_name": "chunk_data"
    },
    "qa": {
        "qdrant_collection_name": "intflex_audit_qa",
        "mongo_database_name": "intflex_audit",
        "mongo_collection_name": "qa"
    }
}
keyword_search_params = {
    "chunk_data": {
        "lucene_index_dir": "./aiknowledge/uploaded_file/indexes/chunk_data",
        "mongo_database_name": "intflex_audit",
        "mongo_collection_name": "chunk_data"
    },
    "qa": {
        "lucene_index_dir": "./aiknowledge/uploaded_file/indexes/qa",
        "mongo_database_name": "intflex_audit",
        "mongo_collection_name": "qa"
    }
}


@st.cache_resource
def get_llm_client():
    openai_config = app_config.get("openai")
    return OpenAILLM(api_key=openai_config["api_key"])


@st.cache_resource
def get_qdrant_client():
    qdrant_config = app_config.get("qdrant")
    return KBQdrantClient(
        url=qdrant_config["url"],
        collection_name=qdrant_config['collection_name']["intflex_audit"],
        embedding_dim=qdrant_config["embedding_dim"]
    )


@st.cache_resource
def get_mongo_client():
    mongo_config = app_config.get("mongo")
    return KBMongoClient(mongo_config['uri'])


@st.cache_resource
def get_hybrid_retriever():
    hybrid_config = app_config.get("cohere")
    return Rerank(api_key=hybrid_config["api_key"])


@st.cache_resource
def get_bm25_retriever():
    return BM25(index_dir=LUCENE_INDEX_DIR_INTFLEX_AUDIT_CHUNK_DATA)


def chatbot_page():

    # Sidebar (Settings)
    with st.sidebar:
        with st.expander("âš™ï¸ è®¾ç½®", expanded=True):
            top_k = st.slider(label="æ£€ç´¢Top K", min_value=1, max_value=10, value=6, step=1)
            # sim_threshold = st.slider(label="Similarity Threshold", min_value=0.0, max_value=1.0, value=0.50, step=0.01)
            # additional_context_length = st.slider(label=":orange[Additional Context Length]", min_value=0, max_value=300, value=50, step=5)
            chat_model_type = st.selectbox(label="æ¨¡å‹é€‰æ‹©", options=["gpt-4o", "gpt-4-turbo", ])
            temperature = st.slider(label="Temperature", min_value=0.0, max_value=1.0, value=0.0, step=0.1,
                                    help="0.0: ç¡®å®šæ€§, 1.0: å¤šæ ·æ€§")
            # is_stream = st.toggle(label="æµå¼è¾“å‡º", value=True)
            # chat_history_len = st.number_input(label="Chat History Length", min_value=0, max_value=20, value=0, step=2, disabled=True)
            # token_and_time_cost_caption = st.toggle(label="æ˜¾ç¤ºè€—æ—¶&ç”¨é‡", value=False)
            word_limit_mode = st.radio("å›ç­”æ¨¡å¼", ["è¯¦ç»†å›ç­”", "ç²¾ç‚¼å›ç­”"], index=0, horizontal=True)

    # Init chat message
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "è¿™é‡Œæ˜¯åˆ™æˆé›¨æ—ï¼Œæ‚¨çš„çŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ï¼Œæƒ³é—®äº›ä»€ä¹ˆï¼Ÿ"}]

    # Display chat session message
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    user_input = st.chat_input("è¯·è¾“å…¥é—®é¢˜...")
    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)

        st.session_state.messages.append({"role": "user", "content": user_input})
    else:
        return

    logger.info(f"User Input: {user_input}")
    logger.info(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")

    llm_client = get_llm_client()
    qdrant_client = get_qdrant_client()
    mongo_client = get_mongo_client()

    with st.chat_message("assistant"):
        ###############################################
        # Query Decomposition & Entity Recognition
        ###############################################
        with st.spinner("é—®é¢˜åˆ†æä¸­..."):
            time1 = time.time()
            query_analysis_list = query_analysis_pipeline(user_input, llm_client)
            time2 = time.time()
            logger.info(f"Query analysis time: {time2 - time1}")

        ###############################################
        # Retrieve Documents for Each Query
        ###############################################
        with st.spinner("æ–‡æ¡£æ£€ç´¢ä¸­..."):
            time1 = time.time()
            reranking_payloads_list, qa_reranking_payloads_list = retrieval_augmentation_pipeline(
                query_analysis_list=query_analysis_list,
                llm_client=llm_client,
                mongo_client=mongo_client,
                qdrant_client=qdrant_client,
                keyword_retriever=get_bm25_retriever(),
                hybrid_retriever=get_hybrid_retriever(),
                vector_search_params=vector_search_params,
                keyword_search_params=keyword_search_params,
                top_k=top_k
            )
            time2 = time.time()
            logger.info(f"Retrieve documents time: {time2 - time1}")
            retrieved_log_str = ""
            print("query_analysis_list: ", query_analysis_list)
            for no_query in range(len(query_analysis_list)):
                retrieved_log_str += f'\t> Query {no_query + 1} | {query_analysis_list[no_query]["query"]} | {" ".join(query_analysis_list[no_query]["entity"])}\n'
                for reranking_payload in reranking_payloads_list[no_query]:
                    retrieved_log_str += f'\t\t>> {reranking_payload["doc_name"]} - {reranking_payload["chunk_seq"]}\n'
            logger.info(f"Retrieved Documents:\n{retrieved_log_str}")

        ###############################################
        # Generate LLM Response
        ###############################################
        # Convert retrieval result to context prompt
        prompt_context = format_context_prompt(reranking_payloads_list)
        qa_prompt_context = format_qa_history_prompt(qa_reranking_payloads_list)
        # AI Message
        with st.spinner("AIæ€è€ƒä¸­..."):
            time1 = time.time()
            word_limit_num = 100 if word_limit_mode == "ç²¾ç‚¼å›ç­”" else None
            response_generator = llm_client.stream_chat_response(
                user_question=user_input,
                context=prompt_context,
                qa_history=qa_prompt_context,
                temperature=temperature,
                model_name=chat_model_type,
                word_limit=word_limit_num
            )
            ai_response = st.write_stream(response_generator)
            ai_response_token_usage = llm_client.stream_chat_token_usage
            time2 = time.time()
            ai_response_log_str = "\t> " + ai_response.replace("\n", "\n\t> ")
            logger.info(f"LLM response time: {time2 - time1}\n")
            logger.info(f"LLM response:\n{ai_response_log_str}")

        st.session_state.messages.append({"role": "assistant", "content": ai_response})
        logger.info("=" * 50)

        ###############################################
        # Display Retrieve Documents
        ###############################################
        with st.expander("ğŸ“– æ–‡æ¡£æ£€ç´¢ç»“æœ", expanded=False):
            for no_query, query_analysis in enumerate(query_analysis_list):
                user_query_type, user_query, entity_list = query_analysis["type"], query_analysis["query"], query_analysis["entity"]
                st.markdown(
                    f'**é—®é¢˜{no_query + 1}**: :orange[{user_query}] '
                    f'('
                    f'é—®é¢˜ç±»å‹: {"æ—¥å¸¸èŠå¤©" if user_query_type == QueryType.CASUAL else "ä¼ä¸šçŸ¥è¯†"}, '
                    f'å…³é”®è¯: {entity_list}'
                    f')'
                )
                if user_query_type == 0:
                    continue
                for no, doc in enumerate(reranking_payloads_list[no_query]):
                    st.write(f':orange[**æ–‡æ¡£æ¥æº{no + 1}: {doc["doc_name"]}**]')
                    st.write(doc["content"][0:200] + "......")
                st.divider()
